# Теория

## About

**Поисковой движок** по данным. Разработан для поиска по **Logstash**.

взято из: [https://habr.com/ru/post/489924/](https://habr.com/ru/post/489924/)

**Apache Lucene** — open-source библиотека полнотекстового поиска, конечно же, с обратным индексом. Elasticsearch использует индексы **Lucene** для хранения данных и поиска.

## Шарды и индексы

**Shard** в Elasticsearch — это логическая единица хранения данных на уровне базы, котрая является отдельным экземпляром **Lucene**.

[Index](https://www.elastic.co/blog/what-is-an-elasticsearch-index) — это одновременно и распределенная база и механизм управления и организации данных, это именно логическое пространство. Индекс содержит один или более шардов, их совокупность и является хранилищем.

Классическое сравнение индекса с другими базами выглядит примерно так.

| Elasticsearch | SQL      | MongoDB      |
| ------------- | -------- | ------------ |
| Index         | Database | Database     |
| Mapping/Type  | Table    | Collection   |
| Field         | Column   | Field        |
| Object(JSON)  | Tuple    | Object(BSON) |

В elastic лучше использовать различные индексы для больших данные: например, создавать отдельный индекс для логов на каждый день — нормальная практика.

По умолчанию количество шардов для индекса будет равным 5, но его всегда возможно изменить в настройках `index.number_of_shards: 1` или с помощью запроса шаблонов индекса.

```javascript
PUT _template/all
{
  "template": "*",
      "settings": {
        "number_of_shards": 1
      }
}
```

Важно управлять этим значением. Всегда принимайте решения с точки зрения параллельной обработки.

Каждый шард способен хранить примерно 232 или 4294967296 записей, это значит, что скорее всего вы упретесь в лимит вашего диска. Однако стоит понимать, все шарды будут участвовать в поиске и если мы будем искать по сотне пустых, потратим время впустую. Если шарды будут слишком большими мы так же будем тратить лишнее время на поиск, а так же операций перемещения и индексации станут очень тяжелыми.

Забегая вперед. Со временем Elasticsearch двигает и изменяет шарды, объединяя дробные и мелкие в большие. Следите за размером ваших шардов, при достижении 10ГБ производительность значительно падает.

## Кластер

За операции поиска и индексации отвечает отдельный инстанс Lucene(_шард_). Для того, чтобы обращаться к распределенной системе шардов, нам необходимо иметь некий координирующий узел, именно он будет принимать запросы и давать задания на запись или получение данных. То есть помимо хранения данных мы выделяем еще один вариант поведения программы — координирование.\


Таким образом мы изначально ориентируемся на два вида узлов — CRUD-узлы и координирующие узлы. Назовем их **data node** и **coordinating node**. У нас есть куча машин объединенных в сеть и все это очень напоминает кластер.

![](<../../../.gitbook/assets/изображение (12).png>)

Каждый запущенный экземпляр **Elasticsearch** является отдельным узлом([**node**](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html)). **Cluster** — это совокупность определенных нод. Когда вы запускаете один экземпляр ваш кластер будет состоять из одной ноды.\
\
&#x20;Для того чтобы объединить узлы в кластер они должны соответствовать ряду требований:

* Ноды должны иметь одинаковую версию
* Имя кластера `cluster.name` в конфигурации должно быть одинаковым

Конфигурация читается из файла **elasticsearch.yml** и переменных среды. Здесь мы можете настроить почти все, что касается неизменных в рантайме свойств ноды.\
\
&#x20;Вы можете поднимать несколько узлов на одной машине, для этого необходимо запускать инстансы из разных директорий файловой системы и в конфигурации указать разные порты `http.port`, по умолчанию порт 9200 для внешнего доступа и 9300 для внутрикластерного соединения. Это может понадобиться для тестирования и проектирования кластера, однако в проде подразумевается использование отдельных машин.

Начиная с версии 6.7 **Elasticsearch** предлагает механизм [управления жизненным циклом](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html). Для этого доступны три типа нод — hot, warm и cold.\
\
Существует рекомендация по выбору аппаратных конфигураций для каждого из типов. Например hot-ноды должны иметь быстрые SSD, для warm и cold достаточно HDD-диска. Оптимальные соотношения память/диск будут следующими:

* hot — 1:30
* warm — 1:100
* cold — 1:500

Для того чтобы определить тип ноды как data node необходимо установить значение в конфигурации `node.data: true`, при этом рекомендуется выделять ноду под один конкретный тип, для повышения стабильности и производительности кластера.

Важнейшим аспектом в использовании распределенных систем является параллельное выполнение задач. Существует популярная модель распределенных вычислений, которая имеет лаконичное название **MapReduce**. И заключается она разделении выполнения задачи на два больших шага:

* **Map** — предварительная обработка данных, формулировка задачи и последующая ее передача выполняющим узлам
* **Reduce** — свертка множества результатов worker-нод в один финальный ответ

![](<../../../.gitbook/assets/изображение (13).png>)

В **Elasticsearch** все узлы неявно являются координирующими. Однако, стоит выделять отдельные coordinating-ноды, не выполняющие других операций. Забегая вперед, такие ноды можно определить, установив значения всех типов в `false`.

